"use strict";(self.webpackChunkcs_notes=self.webpackChunkcs_notes||[]).push([[3862],{3905:(e,n,t)=>{t.d(n,{Zo:()=>c,kt:()=>m});var s=t(67294);function o(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function r(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);n&&(s=s.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,s)}return t}function a(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?r(Object(t),!0).forEach((function(n){o(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function p(e,n){if(null==e)return{};var t,s,o=function(e,n){if(null==e)return{};var t,s,o={},r=Object.keys(e);for(s=0;s<r.length;s++)t=r[s],n.indexOf(t)>=0||(o[t]=e[t]);return o}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(s=0;s<r.length;s++)t=r[s],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(o[t]=e[t])}return o}var _=s.createContext({}),i=function(e){var n=s.useContext(_),t=n;return e&&(t="function"==typeof e?e(n):a(a({},n),e)),t},c=function(e){var n=i(e.components);return s.createElement(_.Provider,{value:n},e.children)},l="mdxType",h={inlineCode:"code",wrapper:function(e){var n=e.children;return s.createElement(s.Fragment,{},n)}},d=s.forwardRef((function(e,n){var t=e.components,o=e.mdxType,r=e.originalType,_=e.parentName,c=p(e,["components","mdxType","originalType","parentName"]),l=i(t),d=o,m=l["".concat(_,".").concat(d)]||l[d]||h[d]||r;return t?s.createElement(m,a(a({ref:n},c),{},{components:t})):s.createElement(m,a({ref:n},c))}));function m(e,n){var t=arguments,o=n&&n.mdxType;if("string"==typeof e||o){var r=t.length,a=new Array(r);a[0]=d;var p={};for(var _ in n)hasOwnProperty.call(n,_)&&(p[_]=n[_]);p.originalType=e,p[l]="string"==typeof e?e:o,a[1]=p;for(var i=2;i<r;i++)a[i]=t[i];return s.createElement.apply(null,a)}return s.createElement.apply(null,t)}d.displayName="MDXCreateElement"},62025:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>_,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>p,toc:()=>i});var s=t(87462),o=(t(67294),t(3905));const r={authors:"kcxain",tags:["NLP","deep learning"]},a="\u8be6\u89e3 Beam Search \u4ee3\u7801\u5b9e\u73b0",p={permalink:"/blog/2022/09/01/beam-search",editUrl:"https://github.com/kcxain/Kcx_Learning/tree/master/blog/2022/09-01-beam-search.md",source:"@site/blog/2022/09-01-beam-search.md",title:"\u8be6\u89e3 Beam Search \u4ee3\u7801\u5b9e\u73b0",description:"Beam Search \u662f\u4e00\u4e2a\u601d\u60f3\u5f88\u7b80\u5355\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u4ee3\u7801\u5b9e\u73b0\u6280\u5de7\u6027\u5f88\u5f3a\u7684\u7b97\u6cd5\uff0c\u4e0d\u540c\u5b9e\u73b0\u65b9\u5f0f\u7684\u6027\u80fd\u53ef\u80fd\u5343\u5dee\u4e07\u522b\u3002",date:"2022-09-01T00:00:00.000Z",formattedDate:"2022\u5e749\u67081\u65e5",tags:[{label:"NLP",permalink:"/blog/tags/nlp"},{label:"deep learning",permalink:"/blog/tags/deep-learning"}],readingTime:6.795,hasTruncateMarker:!0,authors:[{name:"Kcxain",url:"https://github.com/kcxain",email:"kcxain@gmail.com",imageURL:"https://github.com/kcxain.png",key:"kcxain"}],frontMatter:{authors:"kcxain",tags:["NLP","deep learning"]},prevItem:{title:"BERT \u5bb6\u65cf\u5927\u5168\u89e3\u2014\u2014RoBERTa, DeBERTa",permalink:"/blog/2022/11/12/bert-family"},nextItem:{title:"BERT \u539f\u7406\u4e0e\u4ee3\u7801\u89e3\u6790",permalink:"/blog/2022/08/04/bert-code"}},_={authorsImageUrls:[void 0]},i=[{value:"\u51fd\u6570\u89c4\u7ea6",id:"\u51fd\u6570\u89c4\u7ea6",level:2},{value:"\u4ee3\u7801\u89e3\u6790",id:"\u4ee3\u7801\u89e3\u6790",level:2},{value:"\u521d\u59cb\u5316",id:"\u521d\u59cb\u5316",level:3},{value:"\u66f4\u65b0",id:"\u66f4\u65b0",level:3},{value:"\u5b8c\u6574\u4ee3\u7801",id:"\u5b8c\u6574\u4ee3\u7801",level:3},{value:"\u603b\u7ed3",id:"\u603b\u7ed3",level:2}],c={toc:i},l="wrapper";function h(e){let{components:n,...t}=e;return(0,o.kt)(l,(0,s.Z)({},c,t,{components:n,mdxType:"MDXLayout"}),(0,o.kt)("p",null,"Beam Search \u662f\u4e00\u4e2a\u601d\u60f3\u5f88\u7b80\u5355\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u4ee3\u7801\u5b9e\u73b0\u6280\u5de7\u6027\u5f88\u5f3a\u7684\u7b97\u6cd5\uff0c\u4e0d\u540c\u5b9e\u73b0\u65b9\u5f0f\u7684\u6027\u80fd\u53ef\u80fd\u5343\u5dee\u4e07\u522b\u3002"),(0,o.kt)("p",null,"\u5728 ",(0,o.kt)("a",{parentName:"p",href:"https://web.stanford.edu/class/cs224n/"},"Stanford CS 224N | Natural Language Processing with Deep Learning")," \u8bfe\u7a0b\u4f5c\u4e1a ",(0,o.kt)("a",{parentName:"p",href:"/docs/ai/dl-nlp/Assignment4-NMT_with_RNNs"},"A4-NMT with RNNs")," \u4e2d\u5c31\u7528\u5230\u4e86 Beam Search\uff0c\u5b83\u7684",(0,o.kt)("inlineCode",{parentName:"p"},"beam_search"),"\u51fd\u6570\u5b9e\u73b0\u5f97\u975e\u5e38\u5999\uff0c\u5f53\u7136\uff0c\u6280\u5de7\u6027\u4e5f\u5f88\u5f3a\uff0c\u8bfb\u61c2\u5b83\u5e76\u4e0d\u5bb9\u6613\u3002"),(0,o.kt)("p",null,"\u672c\u6587\u5c31\u5177\u4f53\u8bb2\u89e3\u5176\u4e2d\u7684\u5b9e\u73b0\u601d\u8def\u4e0e\u7ec6\u8282\u3002"),(0,o.kt)("h2",{id:"\u51fd\u6570\u89c4\u7ea6"},"\u51fd\u6570\u89c4\u7ea6"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'def beam_search(self, src_sent: List[str], beam_size: int=5, max_decoding_time_step: int=70) -> List[Hypothesis]:\n    """ Given a single source sentence, perform beam search, yielding translations in the target language.\n    @param src_sent (List[str]): a single source sentence (words)\n    @param beam_size (int): beam size\n    @param max_decoding_time_step (int): maximum number of time steps to unroll the decoding RNN\n    @returns hypotheses (List[Hypothesis]): a list of hypothesis, each hypothesis has two fields:\n            value: List[str]: the decoded target sentence, represented as a list of words\n            score: float: the log-likelihood of the target sentence\n    """\n')),(0,o.kt)("p",null,"\u7b80\u5355\u6765\u8bf4\uff0c\u5c31\u662f\u8f93\u5165\u6e90\u53e5\u5b50\uff0cbeam_size\uff0c\u6700\u957f\u957f\u5ea6\uff0c\u671f\u671b\u8f93\u51fa\u6982\u7387\u524d beam_size \u4e2a\u53e5\u5b50\u53ca\u5176\u5bf9\u5e94\u7684\u6982\u7387\u3002"),(0,o.kt)("h2",{id:"\u4ee3\u7801\u89e3\u6790"},"\u4ee3\u7801\u89e3\u6790"),(0,o.kt)("p",null,"\u6211\u5148\u9010\u6b65\u6838\u5fc3\u5206\u6790\uff0c\u6700\u540e\u7ed9\u51fa\u5168\u90e8\u4ee3\u7801"),(0,o.kt)("h3",{id:"\u521d\u59cb\u5316"},"\u521d\u59cb\u5316"),(0,o.kt)("p",null,"\u6574\u4e2a\u4ee3\u7801\u7684\u6838\u5fc3\u5c31\u662f\u7ef4\u62a4",(0,o.kt)("inlineCode",{parentName:"p"},"hypotheses"),"\u548c",(0,o.kt)("inlineCode",{parentName:"p"},"hyp_scores")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"# \u5b58\u653e\u6240\u6709\u5e8f\u5217\u96c6\u5408, \u6700\u5f00\u59cb\u662f<s>\u6807\u7b7e\nhypotheses = [['<s>']]\n# \u6bcf\u4e2a\u5e8f\u5217\u7684\u5206\u6570, \u5927\u5c0f\u4e5f\u5c31\u662f\u5e8f\u5217\u96c6\u5408\u4e2d\u7684\u5e8f\u5217\u6570\u91cf\nhyp_scores = torch.zeros(len(hypotheses), dtype=torch.float, device=self.device)\n# \u5b58\u653e\u5df2\u7ecf\u8ba1\u7b97\u5b8c\u6bd5\u7684\u5e8f\u5217\uff0c\ncompleted_hypotheses = []\n")),(0,o.kt)("h3",{id:"\u66f4\u65b0"},"\u66f4\u65b0"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("strong",{parentName:"li"},"\u9996\u5148\uff0c\u4f7f\u7528\u4e00\u4e2a\u5927\u5faa\u73af\uff0c\u4e00\u6b65\u4e00\u6b65\u89e3\u7801\u751f\u6210\u4e0b\u4e00\u4e2a token\uff0c\u5e76\u5728\u6bcf\u6b65\u7ed3\u675f\u65f6\u9009\u62e9\u5f53\u524d\u5f97\u5206\u524d beam_size \u4e2a\u5e8f\u5217\uff0c\u66f4\u65b0\u5e8f\u5217\u548c\u5f97\u5206"))),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"t = 0\nwhile len(completed_hypotheses) < beam_size and t < max_decoding_time_step:\n    t += 1\n")),(0,o.kt)("p",null,"\u7ec8\u6b62\u6761\u4ef6\u6709\u4e24\u4e2a\uff1a"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"\u8ba1\u7b97\u5b8c\u6bd5\u7684\u5e8f\u5217\u8fbe\u5230\u4e86 beam_size"),(0,o.kt)("li",{parentName:"ul"},"\u5e8f\u5217\u957f\u5ea6\u8fbe\u5230\u4e86 max_decoding_time_step")),(0,o.kt)("p",null,"\u7136\u540e\uff0c\u5728\u5faa\u73af\u5185\u90e8\uff1a"),(0,o.kt)("ol",{start:2},(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("strong",{parentName:"li"},"\u4f7f\u7528",(0,o.kt)("inlineCode",{parentName:"strong"},"torch.expand()"),"\u5c06\u8f93\u5165\u7684 encoding \u590d\u5236\u4e3a\u5f53\u524d\u65f6\u95f4\u6b65\u5e8f\u5217\u7684\u6570\u91cf\uff0c\u8fd9\u662f\u4e3a\u4e86\u5e76\u884c\u751f\u6210"))),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"# \u5e8f\u5217\u7684\u6570\u91cf\nhyp_num = len(hypotheses)\n\n# (hyp_num, src_len, h)\nexp_src_encodings = src_encodings.expand(hyp_num,\n                                         src_encodings.size(1),\n                                         src_encodings.size(2))\n# (hyp_num, src_len, h)\nexp_src_encodings_att_linear = src_encodings_att_linear.expand(hyp_num,                                                   src_encodings_att_linear.size(1),\n                                    src_encodings_att_linear.size(2))\n")),(0,o.kt)("ol",{start:3},(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("strong",{parentName:"li"},"\u6839\u636e\u6bcf\u4e2a\u5e8f\u5217\u7684\u6700\u540e\u4e00\u4e2a token \u548c\u8f93\u5165\uff0c\u5229\u7528",(0,o.kt)("inlineCode",{parentName:"strong"},"step()"),"\u51fd\u6570\u5f97\u5230\u4e0b\u4e00\u6b65 token\uff0c\u5e76\u8ba1\u7b97\u8be5 token \u7684\u6982\u7387"))),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"# \u6bcf\u4e2a\u5e8f\u5217\u7684\u6700\u540e\u4e00\u4e2a\u8bcd\u7684\u5d4c\u5165(hyp_num,e)\ny_tm1 = torch.tensor([self.vocab.tgt[hyp[-1]] for hyp in hypotheses], dtype=torch.long, device=self.device)\ny_t_embed = self.model_embeddings.target(y_tm1)\nx = torch.cat([y_t_embed, att_tm1], dim=-1)\n\n# \u5229\u7528step\u51fd\u6570\u4e0b\u4e00\u6b65\u9884\u6d4b\n(h_t, cell_t), att_t, _  = self.step(x, h_tm1, exp_src_encodings, exp_src_encodings_att_linear, enc_masks=None)\n# \u6ce8\u610f\uff0c\u8fd9\u91cc\u8f93\u5165\u7684\u5927\u5c0f\u4e3a (hyp_num, src_len, h)\n# \u6240\u4ee5\uff0c\u8f93\u51fa\u7684att_t\u5927\u5c0f\u4e3a(hyp_num, h)\n\n# self.target_vocab_projection \u662f\u5c06\u9690\u85cf\u5c42\u9690\u5c04\u5230\u6574\u4e2a\u8bcd\u8868\n# log_p_t \u5c31\u662f\u6bcf\u4e00\u4e2a\u5e8f\u5217\u4e0b\u4e00\u4e2a\u8bcd\u7684\u6982\u7387\n# \u5927\u5c0f\u4e3a(hyp_num, vocab_size)\nlog_p_t = F.log_softmax(self.target_vocab_projection(att_t), dim=-1)\n# \u5269\u4f59\u9700\u8981\u7684\u5e8f\u5217\u6570\u91cf\nlive_hyp_num = beam_size - len(completed_hypotheses)\n")),(0,o.kt)("ol",{start:4},(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("strong",{parentName:"li"},"\u8ba1\u7b97\u4e0d\u540c\u5e8f\u5217\u7684\u5f97\u5206"))),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"# \u8fd9\u5c31\u5f97\u5230\u4e86\u6bcf\u4e2a\u5e8f\u5217\u5728\u6574\u4e2a\u8bcd\u8868\u7684\u5f97\u5206\n# view(-1) \u662f\u4e3a\u4e86\u9009\u53d6\u6240\u6709\u4e2d\u6700\u5927\u7684\n# hyp_scores: (hyp_num, ) -> (hyp_num, 1) -> (hyp_num, vocab_size) -> (hyp_num * vocab_size,) \ncontiuating_hyp_scores = (hyp_scores.unsqueeze(1).expand_as(log_p_t) + log_p_t).view(-1)\n# \u524d k \u4e2a\u6700\u5927\u7684\ntop_cand_hyp_scores, top_cand_hyp_pos = torch.topk(contiuating_hyp_scores, k=live_hyp_num)\n\n# \u627e\u5230\u4e4b\u540e\uff0c\u600e\u4e48\u786e\u5b9a\u8fd9\u524d k \u4e2a\u6700\u5927\u7684\u662f\u54ea\u4e2a\u5e8f\u5217\uff0c\u4ee5\u53ca\u9009\u62e9\u7684\u8bcd\u8868\u4e2d\u7684\u54ea\u4e2a\u8bcd\u5462\uff1f\n# \u7531\u4e8e contiuating_hyp_scores: (hyp_num * vocab_size,), \u6545\u4f5c\u5546\u5c31\u5f97\u5230\u4e86\u5177\u4f53\u7684\u5e8f\u5217\uff0c\u4f59\u6570\u5373\u4e3a\u5bf9\u5e94\u8bcd\u8868\u7684\u8bcd\uff0c\u592a\u79d2\u4e86\uff01\uff01\nprev_hyp_ids = top_cand_hyp_pos // len(self.vocab.tgt) # (live_hyp_num, )\nhyp_word_ids = top_cand_hyp_pos % len(self.vocab.tgt) # (live_hyp_num, )\n")),(0,o.kt)("ol",{start:5},(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("strong",{parentName:"li"},"\u5f97\u5230\u5f53\u524d topk \u7684\u5e8f\u5217\u548c\u5206\u6570"))),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"new_hypotheses = []\nlive_hyp_ids = []\nnew_hyp_scores = []\n# \u4e00\u5171\u5faa\u73af\u4e86 live_hyp_num \u6b21\nfor prev_hyp_id, hyp_word_id, cand_new_hyp_score in zip(prev_hyp_ids, hyp_word_ids, top_cand_hyp_scores):\n    prev_hyp_id = prev_hyp_id.item()\n    hyp_word_id = hyp_word_id.item()\n    cand_new_hyp_score = cand_new_hyp_score.item()\n    # top[i]\u7684\u5e8f\u5217\u548c\u8bcd\n    hyp_word = self.vocab.tgt.id2word[hyp_word_id]\n    new_hyp_sent = hypotheses[prev_hyp_id] + [hyp_word]\n    # \u6709\u7ed3\u675f\u6807\u5fd7\uff0c\u5c31\u76f4\u63a5\u52a0\u8fdb\u5b8c\u6210\u5e8f\u5217\u4e2d\n    if hyp_word == '</s>':\n        completed_hypotheses.append(Hypothesis(value=new_hyp_sent[1:-1],score=cand_new_hyp_score))\n    else:\n        new_hypotheses.append(new_hyp_sent)\n        live_hyp_ids.append(prev_hyp_id)\n        new_hyp_scores.append(cand_new_hyp_score)\n\nif len(completed_hypotheses) == beam_size:\n        break\n")),(0,o.kt)("ol",{start:6},(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("strong",{parentName:"li"},"\u66f4\u65b0",(0,o.kt)("inlineCode",{parentName:"strong"},"hypotheses"),"\u548c",(0,o.kt)("inlineCode",{parentName:"strong"},"hyp_scores"),"\uff0c\u8fdb\u5165\u4e0b\u4e00\u6b65\u5faa\u73af"))),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"hypotheses = new_hypotheses\nhyp_scores = torch.tensor(new_hyp_scores, dtype=torch.float, device=self.device)\n")),(0,o.kt)("h3",{id:"\u5b8c\u6574\u4ee3\u7801"},"\u5b8c\u6574\u4ee3\u7801"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"def beam_search(self, src_sent: List[str], beam_size: int=5, max_decoding_time_step: int=70) -> List[Hypothesis]:\n    \"\"\" Given a single source sentence, perform beam search, yielding translations in the target language.\n    @param src_sent (List[str]): a single source sentence (words)\n    @param beam_size (int): beam size\n    @param max_decoding_time_step (int): maximum number of time steps to unroll the decoding RNN\n    @returns hypotheses (List[Hypothesis]): a list of hypothesis, each hypothesis has two fields:\n            value: List[str]: the decoded target sentence, represented as a list of words\n            score: float: the log-likelihood of the target sentence\n    \"\"\"\n    src_sents_var = self.vocab.src.to_input_tensor([src_sent], self.device)\n\n    src_encodings, dec_init_vec = self.encode(src_sents_var, [len(src_sent)])\n    src_encodings_att_linear = self.att_projection(src_encodings)\n\n    h_tm1 = dec_init_vec\n    att_tm1 = torch.zeros(1, self.hidden_size, device=self.device)\n\n    eos_id = self.vocab.tgt['</s>']\n    hypotheses = [['<s>']]\n    hyp_scores = torch.zeros(len(hypotheses), dtype=torch.float, device=self.device)\n\n    completed_hypotheses = []\n\n    t = 0\n    while len(completed_hypotheses) < beam_size and t < max_decoding_time_step:\n        t += 1\n\n        hyp_num = len(hypotheses)\n\n        exp_src_encodings = src_encodings.expand(hyp_num,\n                                                 src_encodings.size(1),\n                                                 src_encodings.size(2))\n\n        exp_src_encodings_att_linear = src_encodings_att_linear.expand(hyp_num,\n                                                                       src_encodings_att_linear.size(1),\n                                                                       src_encodings_att_linear.size(2))\n\n        y_tm1 = torch.tensor([self.vocab.tgt[hyp[-1]] for hyp in hypotheses], dtype=torch.long, device=self.device)\n        y_t_embed = self.model_embeddings.target(y_tm1)\n\n        x = torch.cat([y_t_embed, att_tm1], dim=-1)\n\n        (h_t, cell_t), att_t, _  = self.step(x, h_tm1,\n                                                  exp_src_encodings, exp_src_encodings_att_linear, enc_masks=None)\n\n        log_p_t = F.log_softmax(self.target_vocab_projection(att_t), dim=-1)\n\n        live_hyp_num = beam_size - len(completed_hypotheses)\n        contiuating_hyp_scores = (hyp_scores.unsqueeze(1).expand_as(log_p_t) + log_p_t).view(-1)\n        top_cand_hyp_scores, top_cand_hyp_pos = torch.topk(contiuating_hyp_scores, k=live_hyp_num)\n        prev_hyp_ids = torch.div(top_cand_hyp_pos, len(self.vocab.tgt), rounding_mode='floor')\n        hyp_word_ids = top_cand_hyp_pos % len(self.vocab.tgt)\n\n        new_hypotheses = []\n        live_hyp_ids = []\n        new_hyp_scores = []\n        for prev_hyp_id, hyp_word_id, cand_new_hyp_score in zip(prev_hyp_ids, hyp_word_ids, top_cand_hyp_scores):\n            prev_hyp_id = prev_hyp_id.item()\n            hyp_word_id = hyp_word_id.item()\n            cand_new_hyp_score = cand_new_hyp_score.item()\n\n            hyp_word = self.vocab.tgt.id2word[hyp_word_id]\n            new_hyp_sent = hypotheses[prev_hyp_id] + [hyp_word]\n            if hyp_word == '</s>':\n                completed_hypotheses.append(Hypothesis(value=new_hyp_sent[1:-1],\n                                                       score=cand_new_hyp_score))\n            else:\n                new_hypotheses.append(new_hyp_sent)\n                live_hyp_ids.append(prev_hyp_id)\n                new_hyp_scores.append(cand_new_hyp_score)\n\n        if len(completed_hypotheses) == beam_size:\n            break\n        live_hyp_ids = torch.tensor(live_hyp_ids, dtype=torch.long, device=self.device)\n        h_tm1 = (h_t[live_hyp_ids], cell_t[live_hyp_ids])\n        att_tm1 = att_t[live_hyp_ids]\n\n        hypotheses = new_hypotheses\n        hyp_scores = torch.tensor(new_hyp_scores, dtype=torch.float, device=self.device)\n\n    if len(completed_hypotheses) == 0:\n        completed_hypotheses.append(Hypothesis(value=hypotheses[0][1:],\n                                               score=hyp_scores[0].item()))\n    completed_hypotheses.sort(key=lambda hyp: hyp.score, reverse=True)\n    return completed_hypotheses\n")),(0,o.kt)("h2",{id:"\u603b\u7ed3"},"\u603b\u7ed3"),(0,o.kt)("p",null,"\u603b\u4f53\u6765\u8bf4\uff0c\u5c31\u662f\u7ef4\u62a4\u4e86\u6bcf\u4e00\u4e2a\u65f6\u95f4\u6b65\u7684\u524d k \u4e2a\u53e5\u5b50\u4ee5\u53ca\u5bf9\u5e94\u7684\u5206\u6570\u3002"),(0,o.kt)("p",null,"\u5728\u5f97\u5230\u4e0b\u4e00\u6b65\u8bcd\u8868\u6982\u7387\u540e\uff0c\u5f53\u524d\u5f20\u91cf\u5f62\u5f0f\u4e3a (k, vocab_size)\uff0c\u5019\u9009\u53e5\u5b50\u5c31\u6709 k * vocab_size \u4e2a\uff0c\u5982\u4f55\u5feb\u901f\u5f97\u5230\u8fd9\u4e48\u591a\u53e5\u5b50\u7684 topk \u5462\uff1f\u8be5\u4ee3\u7801\u6700\u5999\u7684\u5730\u65b9\u5c31\u662f\u5c06\u8be5\u5f20\u91cf\u5c55\u5f00\u4e3a 1 \u7ef4\uff0c\u5feb\u901f\u5f97\u5230 topk \u4e2a\u5e8f\u53f7\uff0c\u518d\u6839\u636e\u5e8f\u53f7\u4e0e vocab_size \u7684\u5546\u548c\u4f59\u5b9a\u4f4d\u5bf9\u5e94\u7684\u53e5\u5b50\u548c\u8bcd\uff0c\u8fd9\u4e00\u6b65\u975e\u5e38\u5999\uff01"))}h.isMDXComponent=!0}}]);